# Learning With Large Datasets

![enter image description here](https://lh3.googleusercontent.com/RCz_hWnN9gzm9fxT3JdzRQjnUeKCaqSsvVNWIU8a3cwqEWlDB8a2tKOpULYF-z-Q9WPlh669VvU3)
![enter image description here](https://lh3.googleusercontent.com/Kb7vFnl9gkCq-ztL7h0qy5Ei-BHzOtPwuJZkb4Z-PlfaJoGsbb3Z1YRPSsnRIOahK_PEKpkrnfq8)

# Stochastic Gradient Descent

![enter image description here](https://lh3.googleusercontent.com/3cv4hjTheGVT9he-7Zoh4dzzzcB2c2uAJHEmWbG1-cQLxcpyN5W4tgYqvs0fNB3FYRDKAOBByuNr)
![enter image description here](https://lh3.googleusercontent.com/frinOEvXDduLM3GTufHFRv1YrezY2u2qo5slmP3xUvsZJpTUX7g8g-JGbJoEie1SLyBemkNTQ3t_)
When we use stochastic gradient descent, firstly we should randomly shuffle the dataset to make it convergences faster.
![enter image description here](https://lh3.googleusercontent.com/l2-b9fu4bxF0Q2wW7UP4H49jZo1yFrh1GyO4zaOCDzqJCj-Vtd9Isi3_EEk2hF69VnoDGeGu3m-6)

# Mini-Batch Gradient Descent

![enter image description here](https://lh3.googleusercontent.com/Jdmxd5W6Vta2c-AkmSYbag2mLj1YxviGX4ptJagoSvLbkWPUA4nuFrPqY64uvQg_di_rIVW85fdR)
![enter image description here](https://lh3.googleusercontent.com/tAqsKrMm-qU2DHxKdoYX2VikDPYJcEr1CkazRONscwbCVdbfkN7DvJ_GpExcTg8kg3eCEKuJJvSk)
Mini-batch gradient descent may run faster than stochastic gradient descent because of vectorization.

# Stochastic Gradient Descent Convergence

![enter image description here](https://lh3.googleusercontent.com/HgT-tOFofXU_UrOabh8zwvwtr52vSV1hw9EAyUr0WdH3p_1zJaLz08yXrbpJj9smXcrqRkXHkNkM)
![enter image description here](https://lh3.googleusercontent.com/6Rx4TCZQHACu0GgDHo9bbJegMFAebbyIscoP6g5m3UvuWDth1xKaftBzfUG8aCMxojVSTtt8w0jV)
figure1. red: try smaller learning rate $\alpha$
figure2. red: try to use more data to average
figure3. maybe 
figure4. try smaller learning rate $\alpha$

# Online Learning



# Map Reduce and Data Parallelism
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3NDgzNjMwODgsMTE2OTcxMDA4NSw4Mj
E4MTkxNTIsLTQ2MzgwNDYzMF19
-->