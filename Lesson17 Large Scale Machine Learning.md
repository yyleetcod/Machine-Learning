# Learning With Large Datasets

![enter image description here](https://lh3.googleusercontent.com/RCz_hWnN9gzm9fxT3JdzRQjnUeKCaqSsvVNWIU8a3cwqEWlDB8a2tKOpULYF-z-Q9WPlh669VvU3)
![enter image description here](https://lh3.googleusercontent.com/Kb7vFnl9gkCq-ztL7h0qy5Ei-BHzOtPwuJZkb4Z-PlfaJoGsbb3Z1YRPSsnRIOahK_PEKpkrnfq8)

# Stochastic Gradient Descent

![enter image description here](https://lh3.googleusercontent.com/3cv4hjTheGVT9he-7Zoh4dzzzcB2c2uAJHEmWbG1-cQLxcpyN5W4tgYqvs0fNB3FYRDKAOBByuNr)
![enter image description here](https://lh3.googleusercontent.com/frinOEvXDduLM3GTufHFRv1YrezY2u2qo5slmP3xUvsZJpTUX7g8g-JGbJoEie1SLyBemkNTQ3t_)
When we use stochastic gradient descent, firstly we should randomly shuffle the dataset to make it convergences faster.
![enter image description here](https://lh3.googleusercontent.com/l2-b9fu4bxF0Q2wW7UP4H49jZo1yFrh1GyO4zaOCDzqJCj-Vtd9Isi3_EEk2hF69VnoDGeGu3m-6)

# Mini-Batch Gradient Descent

![enter image description here](https://lh3.googleusercontent.com/Jdmxd5W6Vta2c-AkmSYbag2mLj1YxviGX4ptJagoSvLbkWPUA4nuFrPqY64uvQg_di_rIVW85fdR)
![enter image description here](https://lh3.googleusercontent.com/tAqsKrMm-qU2DHxKdoYX2VikDPYJcEr1CkazRONscwbCVdbfkN7DvJ_GpExcTg8kg3eCEKuJJvSk)
Mini-batch gradient descent may run faster than c

# Stochastic Gradient Descent Convergence



# Online Learning



# Map Reduce and Data Parallelism
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTk4NjY1NzE5NCwtNDYzODA0NjMwXX0=
-->